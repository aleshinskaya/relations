{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../../context-mediation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20245480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd95185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "device = \"cuda\"\n",
    "config = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config, revision=\"float16\", low_cpu_mem_usage=True)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2643226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data\n",
    "\n",
    "counterfact = data.load_dataset(\"counterfact\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfact[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c92b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "samples_by_relation = defaultdict(list)\n",
    "for sample in counterfact:\n",
    "    relation_id = sample[\"source\"][\"requested_rewrite\"][\"relation_id\"]\n",
    "    samples_by_relation[relation_id].append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ded5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_by_relation), {r: len(s) for r, s in samples_by_relation.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0efeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import tokenizer_utils\n",
    "\n",
    "import baukit\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "preds_by_relation = defaultdict(list)\n",
    "logp_by_relation = defaultdict(list)\n",
    "with counterfact.formatted_as(\"torch\"):\n",
    "    loader = DataLoader(counterfact, batch_size=32, shuffle=False)\n",
    "    for batch in tqdm(loader):\n",
    "        inputs = tokenizer(\n",
    "            batch[\"prompt\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        ).to(device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_idx = torch.arange(len(batch[\"prompt\"]))\n",
    "        prompt_idx = inputs.attention_mask.sum(dim=-1) - 1\n",
    "        topk = torch.log_softmax(outputs.logits, dim=-1)[batch_idx, prompt_idx].topk(dim=-1, k=5)\n",
    "        ids = topk.indices\n",
    "        logps = topk.values\n",
    "        tokens = tokenizer_utils.batch_convert_ids_to_tokens(ids.tolist(), tokenizer)\n",
    "\n",
    "        for rid, preds, logp in zip(\n",
    "            batch[\"source\"][\"requested_rewrite\"][\"relation_id\"],\n",
    "            tokens,\n",
    "            logps.cpu(),\n",
    "        ):\n",
    "            preds_by_relation[rid].append(preds)\n",
    "            logp_by_relation[rid].append(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_by_relation[\"P264\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c86101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import estimate\n",
    "\n",
    "layer = 15\n",
    "n_train = 5\n",
    "n_test = 150\n",
    "\n",
    "\n",
    "relation_templates = {\n",
    "    \"P103\": \"{} speaks the language of\",\n",
    "    \"P140\": \"{} follows the religion of\",\n",
    "    \"P740\": \"{} originates from\",\n",
    "    \"P190\": \"{} has a sister city named \\\"\",\n",
    "    \"P178\": \"{} was originally developed by\",\n",
    "    \"P176\": \"{} is developed by the company\",\n",
    "    \"P413\": \"In their sport, {} plays the position of\",\n",
    "    \"P39\": \"{} has the occupation of\",\n",
    "    \"P407\": \"{} is written in the language of\",\n",
    "    \"P101\": \"{} is associated with the field of\",\n",
    "    \"P1412\": \"{} speaks the language of\",\n",
    "    \"P27\": \"{} is originally from the country of\",\n",
    "    \"P106\": \"{} has the occupation of\",\n",
    "    \"P136\": \"{} is associated with the genre of\",\n",
    "    \"P159\": \"{} is based in the city of\",\n",
    "    \"P276\": \"{} is located in the city of\",\n",
    "    \"P36\": \"{} has the capitol city of\",\n",
    "}\n",
    "\n",
    "\n",
    "accs_by_relation = defaultdict(dict)\n",
    "for rid, samples in samples_by_relation.items():\n",
    "    print(f\"---- {rid} ----\")\n",
    "\n",
    "    # Only use examples for which the model encodes the correct relation.\n",
    "    samples = [\n",
    "        s\n",
    "        for i, s in sorted(\n",
    "            enumerate(samples),\n",
    "            key=lambda x: logp_by_relation[rid][x[0]][0].item(),\n",
    "            reverse=True,\n",
    "        )\n",
    "        if any(\n",
    "            pred.strip(\"ĠĊ \").lower()\n",
    "            in\n",
    "            s[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"].strip().lower()\n",
    "            for pred in preds_by_relation[rid][i][:1]\n",
    "            if pred.strip(\"ĠĊ \").lower()\n",
    "        )\n",
    "    ]\n",
    "    print(f\"{len(samples)} known samples\")\n",
    "\n",
    "    # When picking training examples, choose a diverse set of labels.\n",
    "    trains = []\n",
    "    tests = []\n",
    "    seen = set()\n",
    "    for sample in samples:\n",
    "        label = sample[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        if len(trains) >= n_train:\n",
    "            if len(tests) < n_test:\n",
    "                tests.append(sample)\n",
    "            continue\n",
    "        elif label not in seen:\n",
    "            trains.append(sample)\n",
    "            seen.add(label)\n",
    "        else:\n",
    "            tests.append(sample)\n",
    "\n",
    "    # Pick best relation text via heuristic\n",
    "    if rid in relation_templates:\n",
    "        relation = relation_templates[rid]\n",
    "    else:\n",
    "        #         continue\n",
    "        # Pick by heuristic\n",
    "        rs0 = [\n",
    "            sample[\"source\"][\"requested_rewrite\"][\"prompt\"]\n",
    "            for sample in samples\n",
    "        ]\n",
    "        # Always prefer one that puts the subject first:\n",
    "        rs1 = [r for r in rs0 if r.startswith(\"{}\")]\n",
    "        # Then, prefer one with no special punctuation:\n",
    "        rs2 = [r for r in rs1 if not any(x in r for x in \"?,:;.\")]\n",
    "        # Then prefer the longest one:\n",
    "        rs3 = sorted(rs2, key=lambda r: len(r), reverse=True)\n",
    "        relation = None\n",
    "        for rs in (rs3, rs2, rs1, rs0):\n",
    "            if rs:\n",
    "                relation = rs[0]\n",
    "                break\n",
    "        assert relation is not None\n",
    "\n",
    "    batch = [\n",
    "        (\n",
    "            train[\"source\"][\"requested_rewrite\"][\"subject\"],\n",
    "            train[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        )\n",
    "        for train in trains\n",
    "    ]\n",
    "    print(relation)\n",
    "    print(batch)\n",
    "    print(len(tests), \"test examples\")\n",
    "#     continue\n",
    "#     print([t[\"source\"][\"requested_rewrite\"][\"prompt\"] for t in trains])\n",
    "#     continue\n",
    "\n",
    "    operator_a, _ = estimate.relation_operator_from_batch(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        relation=relation,\n",
    "        samples=batch,\n",
    "        layer=layer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    correct_by_k = defaultdict(int)\n",
    "    for test in tqdm(tests, desc=\"test\"):\n",
    "        rr = test[\"source\"][\"requested_rewrite\"]\n",
    "        subject = rr[\"subject\"]\n",
    "        expected = rr[\"target_true\"][\"str\"]\n",
    "\n",
    "        preds = operator_a(subject, device=device, return_top_k=5)\n",
    "        for k in (1, 3, 5):\n",
    "            actuals = [p.strip(\"ĠĊ \").lower() for p in preds[:k]]\n",
    "            correct_by_k[k] += any(\n",
    "                expected.lower().strip().startswith(actual)\n",
    "                for actual in actuals\n",
    "            )\n",
    "\n",
    "    for k in (1, 3, 5):\n",
    "        accuracy = correct_by_k[k] / len(tests)\n",
    "        print(f\"top-{k} accuracy: {accuracy:.2f}\")\n",
    "        accs_by_relation[rid][k] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfd67d",
   "metadata": {},
   "source": [
    "Cases to analyze:\n",
    "- P176: \"is developed by\"; how come we get zero accuracy?\n",
    "- P413: Which prompt is a good one?\n",
    "- P190: Sister city...why so bad?\n",
    "- P136: What is going on with this relation? What is it supposed to be?\n",
    "- P1412: Why is this different from the other language speaking one?\n",
    "- P140, P103: What is the top-1 token?\n",
    "\n",
    "**Nasty**: Copy paste above to compute baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "baseline_by_relation = {}\n",
    "for rid, samples in samples_by_relation.items():\n",
    "    print(f\"---- {rid} ----\")\n",
    "\n",
    "    # Only use examples for which the model encodes the correct relation.\n",
    "    samples = [\n",
    "        s\n",
    "        for i, s in sorted(\n",
    "            enumerate(samples),\n",
    "            key=lambda x: logp_by_relation[rid][x[0]][0].item(),\n",
    "            reverse=True,\n",
    "        )\n",
    "        if any(\n",
    "            pred.strip(\"ĠĊ \").lower()\n",
    "            in\n",
    "            s[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"].lower()\n",
    "            for pred in preds_by_relation[rid][i][:1]\n",
    "            if pred.strip(\"ĠĊ \").lower()\n",
    "        )\n",
    "    ]\n",
    "    print(f\"{len(samples)} known samples\")\n",
    "\n",
    "    # When picking training examples, choose a diverse set of labels.\n",
    "    trains = []\n",
    "    tests = []\n",
    "    seen = set()\n",
    "    for sample in samples:\n",
    "        label = sample[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        if len(trains) >= n_train:\n",
    "            if len(tests) < n_test:\n",
    "                tests.append(sample)\n",
    "            continue\n",
    "        elif label not in seen:\n",
    "            trains.append(sample)\n",
    "            seen.add(label)\n",
    "        else:\n",
    "            tests.append(sample)\n",
    "\n",
    "    # Pick best relation text via heuristic\n",
    "    if rid in relation_templates:\n",
    "        relation = relation_templates[rid]\n",
    "    else:\n",
    "        #         continue\n",
    "        # Pick by heuristic\n",
    "        rs0 = [\n",
    "            sample[\"source\"][\"requested_rewrite\"][\"prompt\"]\n",
    "            for sample in samples\n",
    "        ]\n",
    "        # Always prefer one that puts the subject first:\n",
    "        rs1 = [r for r in rs0 if r.startswith(\"{}\")]\n",
    "        # Then, prefer one with no special punctuation:\n",
    "        rs2 = [r for r in rs1 if not any(x in r for x in \"?,:;.\")]\n",
    "        # Then prefer the longest one:\n",
    "        rs3 = sorted(rs2, key=lambda r: len(r), reverse=True)\n",
    "        relation = None\n",
    "        for rs in (rs3, rs2, rs1, rs0):\n",
    "            if rs:\n",
    "                relation = rs[0]\n",
    "                break\n",
    "        assert relation is not None\n",
    "\n",
    "    batch = [\n",
    "        (\n",
    "            train[\"source\"][\"requested_rewrite\"][\"subject\"],\n",
    "            train[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        )\n",
    "        for train in trains\n",
    "    ]\n",
    "    print(relation)\n",
    "    print(batch)\n",
    "    print(len(tests), \"test examples\")\n",
    "\n",
    "    label = Counter([sample[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"] for sample in samples]).most_common()[0][0]\n",
    "    print(\"Majority=\", label)\n",
    "    correct = 0\n",
    "    for test in tqdm(samples, desc=\"test\"):\n",
    "        expected = test[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        correct += expected.strip().lower() == label.strip().lower()\n",
    "    \n",
    "    accuracy = correct / len(samples)\n",
    "    print(f\"accuracy={accuracy:.2f}\")\n",
    "    baseline_by_relation[rid] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "xys = sorted(accs_by_relation.items(), key=lambda kv: kv[-1][3], reverse=True)\n",
    "\n",
    "banned = {\n",
    "    # Too few examples\n",
    "    \"P463\",\n",
    "    \"P264\",\n",
    "    # Really weird; mostly jazz? What is this one?\n",
    "    \"P136\",\n",
    "}\n",
    "xys = [(x, y) for x, y in xys if x not in banned]\n",
    "\n",
    "xs = [rid for rid, _ in xys]\n",
    "ys = [scores[3] for _, scores in xys]\n",
    "zs = [baseline_by_relation[rid] for rid in xs]\n",
    "\n",
    "x = np.arange(len(xs))\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "ours = ax.bar(x - width / 2, ys, width, label=\"Estimate J/b\")\n",
    "baseline = ax.bar(x + width / 2, zs, width, label=\"Majority vote\")\n",
    "ax.set_xticks(x, xs)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.title(\"top-3 accuracy by relation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36defb21",
   "metadata": {},
   "source": [
    "# What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa950ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_by_relation = {}\n",
    "for rid, samples in samples_by_relation.items():\n",
    "    targets_by_relation[rid] = {\n",
    "        sample[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        for sample in samples\n",
    "    }\n",
    "{rid: targets for rid, targets in targets_by_relation.items()}[\"P39\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df658ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_by_relation = {}\n",
    "for rid, samples in samples_by_relation.items():\n",
    "    prompts_by_relation[rid] = [\n",
    "        sample[\"source\"][\"requested_rewrite\"][\"prompt\"]\n",
    "        for sample in samples\n",
    "    ]\n",
    "{rid: prompts for rid, prompts in prompts_by_relation.items()}[\"P176\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rid = \"P176\"\n",
    "# relation = \"{} was developed by\"\n",
    "# trained_on = [\n",
    "#     ('Microsoft Band', 'Microsoft'),\n",
    "#     ('Kindle Fire', 'Amazon'),\n",
    "#     ('Sony NEX-5', 'Sony'),\n",
    "#     ('Chromecast', 'Google'),\n",
    "#     ('Nintendo Entertainment System', 'Nintendo'),\n",
    "# ]\n",
    "\n",
    "\n",
    "rid = \"P1412\"\n",
    "relation = \"{} primarily spoke the language of\"\n",
    "trained_on = [\n",
    "    ('Juan Bautista de Anza', 'Spanish'),\n",
    "    ('Glamourina', 'Ukrainian'),\n",
    "    ('Francesc Eiximenis', 'Catalan'),\n",
    "    ('Milo Manara', 'Italian'),\n",
    "    ('Aleksandar Zograf', 'Serbian'),\n",
    "]\n",
    "\n",
    "tests = [\n",
    "    (\n",
    "        s[\"source\"][\"requested_rewrite\"][\"subject\"],\n",
    "        s[\"source\"][\"requested_rewrite\"][\"target_true\"][\"str\"],\n",
    "    )\n",
    "    for s in samples_by_relation[rid]\n",
    "]\n",
    "\n",
    "operator, _ = estimate.relation_operator_from_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    trained_on,\n",
    "    relation,\n",
    "    device=device)\n",
    "\n",
    "# Logit lens for b.\n",
    "logits = model.lm_head(model.transformer.ln_f(operator.bias[None]))\n",
    "ids = logits.topk(k=5, dim=-1).indices\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "print(\"LOGIT LENS\", tokens)\n",
    "\n",
    "correct = 0\n",
    "for subject, target in tests:\n",
    "    predictions = operator(subject, device=device)\n",
    "    print(subject, f\"preds={predictions}\", f\"target={target}\")\n",
    "    correct += any(p.strip(\"Ġ \") in target for p in predictions)\n",
    "print(f\"{correct / len(tests):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4c6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
